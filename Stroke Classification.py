# -*- coding: utf-8 -*-
"""Neural_Netwokrs_Fina.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AgOKxeRAcuPCpvpGil0K0wNbpVu64buO
"""

import os
import cv2
import json
import time
import torch
import random
import shutil
import numpy as np
import pandas as pd
from PIL import Image
import torch.nn as nn
import seaborn as sns
import albumentations as A
import torch.optim as optim
from torchvision import models
from google.colab import drive
import matplotlib.pyplot as plt
from collections import Counter
from albumentations.pytorch import ToTensorV2
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import StratifiedShuffleSplit
from torch.utils.data import Dataset, ConcatDataset, DataLoader, Subset
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Mount Google Drive
drive.mount('/content/drive', force_remount=True)

# === Kaggle Auth ===
kaggle_api = {
    "username": "lazarosalexandridis",
    "key": "h32h5h6h78j67k5ik489s765v6g78993"
}

os.makedirs("/root/.kaggle", exist_ok=True)
with open("/root/.kaggle/kaggle.json", "w") as file:
    json.dump(kaggle_api, file)

!chmod 600 /root/.kaggle/kaggle.json
!kaggle datasets download -d orvile/inme-veri-seti-stroke-dataset
!unzip -o inme-veri-seti-stroke-dataset.zip -d ./stroke_data

# === Î‘ÏÏ‡ÎµÎ¯Î± ÎµÎ¹ÏƒÏŒÎ´Î¿Ï… ===
bleeding_files = []
ischemia_files = []
healthy_files = []

# === Bleeding ===
bleeding_path = './stroke_data/Ä°NME VERÄ° SETÄ°/Kanama'
for file in os.listdir(os.path.join(bleeding_path, 'PNG')):
    bleeding_files.append(os.path.join(bleeding_path, 'PNG', file))

# === Ischemia ===
ischemia_path = './stroke_data/Ä°NME VERÄ° SETÄ°/Ä°skemi'
for file in os.listdir(os.path.join(ischemia_path, 'PNG')):
    ischemia_files.append(os.path.join(ischemia_path, 'PNG', file))

# === Healthy (ÎœÏŒÎ½Î¿ 1100 random ÎµÎ¹ÎºÏŒÎ½ÎµÏ‚) ===
healthy_path = './stroke_data/Ä°NME VERÄ° SETÄ°/Ä°nme Yok'
healthy_all = os.listdir(healthy_path)
random.seed(42)  # Î³Î¹Î± ÏƒÏ„Î±Î¸ÎµÏÏŒÏ„Î·Ï„Î±
healthy_sample = random.sample(healthy_all, 1100)
for file in healthy_sample:
    healthy_files.append(os.path.join(healthy_path, file))

# === Î•ÎºÏ„ÏÏ€Ï‰ÏƒÎ· Î¼ÎµÏ„ÏÎ®ÏƒÎµÏ‰Î½ ===
print(f"\nğŸ“Š Bleeding: {len(bleeding_files)} photos")
print(f"ğŸ“Š Ischemia: {len(ischemia_files)} photos")
print(f"ğŸ“Š Healthy: {len(healthy_files)} photos")

# Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬
bleeding_count = len(bleeding_files)
ischemia_count = len(ischemia_files)
healthy_full_count = len(healthy_all)
healthy_sampled_count = len(healthy_files)

# Î”ÎµÎ´Î¿Î¼Î­Î½Î± Î³Î¹Î± Ï„Î± Î´ÏÎ¿ plots
categories_1 = ['Bleeding', 'Ischemia', 'Healthy (Full)']
values_1 = [bleeding_count, ischemia_count, healthy_full_count]

categories_2 = ['Bleeding', 'Ischemia', 'Healthy (Sampled)']
values_2 = [bleeding_count, ischemia_count, healthy_sampled_count]

colors = ['#1f77b4'] * 3  # ÎœÏ€Î»Îµ Ï‡ÏÏÎ¼Î±

fig, axs = plt.subplots(1, 2, figsize=(12, 6), sharey=True)

# Î”Î¹Î¬Î³ÏÎ±Î¼Î¼Î± 1
bars1 = axs[0].bar(categories_1, values_1, color=colors, width=0.5)
axs[0].set_title("Î”Î¹Î¬Î³ÏÎ±Î¼Î¼Î± 1: Full Dataset")
axs[0].set_ylabel("Î‘ÏÎ¹Î¸Î¼ÏŒÏ‚ Î•Î¹ÎºÏŒÎ½Ï‰Î½")
axs[0].grid(axis='y', linestyle='--', alpha=0.7)
for bar in bars1:
    height = bar.get_height()
    axs[0].annotate(f'{height}',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3),
                    textcoords="offset points",
                    ha='center', va='bottom')

# Î”Î¹Î¬Î³ÏÎ±Î¼Î¼Î± 2
bars2 = axs[1].bar(categories_2, values_2, color=colors, width=0.5)
axs[1].set_title("Î”Î¹Î¬Î³ÏÎ±Î¼Î¼Î± 2: Sampled Dataset")
axs[1].grid(axis='y', linestyle='--', alpha=0.7)
for bar in bars2:
    height = bar.get_height()
    axs[1].annotate(f'{height}',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3),
                    textcoords="offset points",
                    ha='center', va='bottom')

plt.suptitle("Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬ Î•Î¹ÎºÏŒÎ½Ï‰Î½ Î‘Î½Î¬ ÎšÎ±Ï„Î·Î³Î¿ÏÎ¯Î±")
plt.tight_layout(rect=[0, 0, 1, 0.95])
plt.show()

def print_class_distribution(labels, label_map, title=""):
    counts = Counter(labels)
    print(f"{title}")
    for label in sorted(counts):
        print(f"    {label_map[label]}: {counts[label]}")

def crop_black_areas(image_path):
    # Load image in grayscale
    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if image is None:
        raise ValueError(f"Failed to load image: {image_path}")

    # Convert to uint8 if needed
    image = image.astype(np.uint8)
    original_shape = image.shape  # (height, width)

    # Threshold to find non-black regions
    _, thresh = cv2.threshold(image, 10, 255, cv2.THRESH_BINARY)

    # Find contours
    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    if not contours:
        cropped = image.copy()
        cropped_shape = original_shape
    else:
        x, y, w, h = cv2.boundingRect(max(contours, key=cv2.contourArea))
        padding = 10
        x = max(x - padding, 0)
        y = max(y - padding, 0)
        w = min(w + 2 * padding, image.shape[1] - x)
        h = min(h + 2 * padding, image.shape[0] - y)
        cropped = image[y:y+h, x:x+w]
        cropped_shape = cropped.shape

    return cropped

def split_and_save(bleeding_files, ischemia_files, healthy_files):
    output_dir = '/content/stroke_data/split_dataset'
    categories = ['Bleeding', 'Ischemia', 'Healthy']
    category_map = {0: 'Bleeding', 1: 'Ischemia', 2: 'Healthy'}

    # Remove duplicates
    bleeding_files = list(set(bleeding_files))
    ischemia_files = list(set(ischemia_files))
    healthy_files = list(set(healthy_files))

    all_files = bleeding_files + ischemia_files + healthy_files
    labels = [0] * len(bleeding_files) + [1] * len(ischemia_files) + [2] * len(healthy_files)

    print(f"ğŸ“¸ Total unique photos: {len(all_files)}")
    print_class_distribution(labels, category_map, "ğŸ“Š Î£ÏÎ½Î¿Î»Î¿ dataset:")

    # Stratified K-Fold
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    fold_files = [[] for _ in range(5)]
    fold_labels = [[] for _ in range(5)]
    for fold_idx, (_, val_idx) in enumerate(skf.split(all_files, labels)):
        fold_files[fold_idx] = [all_files[i] for i in val_idx]
        fold_labels[fold_idx] = [labels[i] for i in val_idx]
        print(f"\nğŸ“‚ Fold {fold_idx}: {len(fold_files[fold_idx])} photos")
        print_class_distribution(fold_labels[fold_idx], category_map, f"ğŸ“Š Fold {fold_idx} class distribution:")

    # Clear output directory
    if os.path.exists(output_dir):
        shutil.rmtree(output_dir)

    # Create folders for each fold
    for fold_idx in range(5):
        for category in categories:
            os.makedirs(os.path.join(output_dir, f'fold_{fold_idx}', category), exist_ok=True)

    # Copy and crop images to respective folders
    for fold_idx in range(5):
        fold_dir = os.path.join(output_dir, f'fold_{fold_idx}')
        for file_path, label in zip(fold_files[fold_idx], fold_labels[fold_idx]):
            category = category_map[label]
            filename = os.path.basename(file_path)
            dest = os.path.join(fold_dir, category, filename)
            try:
                # Crop the image
                cropped_image = crop_black_areas(file_path)
                # Save the cropped image
                cv2.imwrite(dest, cropped_image)
            except Exception as e:
                print(f"âŒ Î£Ï†Î¬Î»Î¼Î± ÏƒÏ„Î¿ photo {file_path}: {e}")

        # Integrity check
        for category in categories:
            category_path = os.path.join(fold_dir, category)
            num_photos = len([f for f in os.listdir(category_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])
            expected = sum(1 for l in fold_labels[fold_idx] if category_map[l] == category)
            if num_photos != expected:
                print(f"âŒ Î£Ï†Î¬Î»Î¼Î± ÏƒÏ„Î¿Î½ Ï†Î¬ÎºÎµÎ»Î¿ [Fold {fold_idx}] {category}: "
                      f"Î‘Î½Î±Î¼ÎµÎ½ÏŒÎ¼ÎµÎ½Î± {expected} Î±ÏÏ‡ÎµÎ¯Î±, Î²ÏÎ­Î¸Î·ÎºÎ±Î½ {num_photos}")

split_and_save(bleeding_files, ischemia_files, healthy_files)

def display_random_image_from_each_fold(dataset_dir='/content/stroke_data/split_dataset'):
    # Î›Î¯ÏƒÏ„Î± Î¼Îµ Ï„Î¿Ï…Ï‚ Ï†Î±ÎºÎ­Î»Î¿Ï…Ï‚ fold (fold_0, fold_1, ..., fold_4)
    folds = [f'fold_{i}' for i in range(5)]
    categories = ['Bleeding', 'Ischemia', 'Healthy']

    # Î¡ÏÎ¸Î¼Î¹ÏƒÎ· Ï„Î¿Ï… subplot Î³Î¹Î± ÎµÎ¼Ï†Î¬Î½Î¹ÏƒÎ· 5 ÎµÎ¹ÎºÏŒÎ½Ï‰Î½ (1 Î±Ï€ÏŒ ÎºÎ¬Î¸Îµ fold)
    fig, axes = plt.subplots(1, 5, figsize=(15, 3))
    axes = axes.flatten()

    for idx, fold in enumerate(folds):
        fold_dir = os.path.join(dataset_dir, fold)
        if not os.path.exists(fold_dir):
            print(f"âŒ ÎŸ Ï†Î¬ÎºÎµÎ»Î¿Ï‚ {fold_dir} Î´ÎµÎ½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹")
            continue

        # Î£Ï…Î»Î»Î¿Î³Î® ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ ÎµÎ¹ÎºÏŒÎ½Ï‰Î½ Î±Ï€ÏŒ Ï„Î¹Ï‚ ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯ÎµÏ‚ Ï„Î¿Ï… fold
        all_images = []
        for category in categories:
            category_dir = os.path.join(fold_dir, category)
            if os.path.exists(category_dir):
                images = [os.path.join(category_dir, f) for f in os.listdir(category_dir)
                          if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
                all_images.extend(images)

        if not all_images:
            print(f"âŒ ÎšÎ±Î¼Î¯Î± ÎµÎ¹ÎºÏŒÎ½Î± Î´ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ ÏƒÏ„Î¿ {fold}")
            axes[idx].set_visible(False)
            continue

        # Î•Ï€Î¹Î»Î¿Î³Î® Ï„Ï…Ï‡Î±Î¯Î±Ï‚ ÎµÎ¹ÎºÏŒÎ½Î±Ï‚
        random_image_path = random.choice(all_images)

        # Î¦ÏŒÏÏ„Ï‰ÏƒÎ· ÎºÎ±Î¹ ÎµÎ¼Ï†Î¬Î½Î¹ÏƒÎ· Ï„Î·Ï‚ ÎµÎ¹ÎºÏŒÎ½Î±Ï‚
        try:
            image = Image.open(random_image_path).convert('L')  # Î¦ÏŒÏÏ„Ï‰ÏƒÎ· ÏƒÎµ grayscale
            axes[idx].imshow(image, cmap='gray')
            axes[idx].set_title(f'Fold {idx}')
            axes[idx].axis('off')
        except Exception as e:
            print(f"âŒ Î£Ï†Î¬Î»Î¼Î± ÎºÎ±Ï„Î¬ Ï„Î· Ï†ÏŒÏÏ„Ï‰ÏƒÎ· Ï„Î·Ï‚ ÎµÎ¹ÎºÏŒÎ½Î±Ï‚ {random_image_path}: {e}")
            axes[idx].set_visible(False)

    plt.tight_layout()
    plt.show()


display_random_image_from_each_fold()

base_path = '/content/stroke_data/split_dataset'

# Î¦Î¬ÎºÎµÎ»Î¿Î¹ Ï€Î¿Ï… Ï€ÎµÏÎ¹Î­Ï‡Î¿Ï…Î½ Ï„Î± ÏƒÏÎ½Î¿Î»Î± Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½
folders = [f'fold_{i}' for i in range(5)]
categories = ['Bleeding', 'Ischemia', 'Healthy']

# Î§ÏÏÎ¼Î±Ï„Î± (Î¼Ï€Î»Îµ Î±Ï€Î¿Ï‡ÏÏÏƒÎµÎ¹Ï‚)
colors = ['#99ccff', '#6699ff', '#3366ff']  # Bleeding, Ischemia, Healthy

# ÎœÎ±Î¶ÎµÏÎ¿Ï…Î¼Îµ Ï„Î± Ï€Î»Î®Î¸Î· ÎµÎ¹ÎºÏŒÎ½Ï‰Î½
distribution = []
for folder in folders:
    counts = []
    for category in categories:
        category_path = os.path.join(base_path, folder, category)
        count = len([
            f for f in os.listdir(category_path)
            if os.path.isfile(os.path.join(category_path, f)) and f.lower().endswith(('.png', '.jpg', '.jpeg'))
        ])
        counts.append(count)
    distribution.append(counts)

distribution = np.array(distribution)

# === Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Î´Î¹Î±Î³ÏÎ¬Î¼Î¼Î±Ï„Î¿Ï‚ ===
x = np.arange(len(folders))  # x-Î¬Î¾Î¿Î½Î±Ï‚ Î³Î¹Î± ÎºÎ¬Î¸Îµ group
bar_width = 0.2
spacing = 0.05

fig, ax = plt.subplots(figsize=(12, 6))

for i in range(len(categories)):
    ax.bar(x + (i - 1) * (bar_width + spacing), distribution[:, i], width=bar_width, color=colors[i], label=categories[i])

# Î¡Ï…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚ Î´Î¹Î±Î³ÏÎ¬Î¼Î¼Î±Ï„Î¿Ï‚
ax.set_xticks(x)
ax.set_xticklabels([f.replace('train/', 'Fold ').replace('_', ' ') if 'fold_' in f else f.capitalize() for f in folders])
ax.set_xlabel('Dataset Partition')
ax.set_ylabel('Number of Images')
ax.set_title('Image Distribution per Partition and Category')
ax.legend(title='Category')
plt.tight_layout()
plt.show()

"""# **Common Functions and Classes**  """

# Mount Google Drive
drive.mount('/content/drive', force_remount=True)

# Set up local dataset
local_data_dir = '/content/stroke_data'
if not os.path.exists(local_data_dir):
    shutil.copytree('/content/drive/MyDrive/stroke_data', local_data_dir)
    print(f"Dataset copied to {local_data_dir}")

# --- Stratified Split ---
def stratified_split(dataset, val_ratio):
    labels = []
    for d in dataset.datasets:
        labels.extend(d.labels)
    labels = np.array(labels)
    indices = np.arange(len(labels))

    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio, random_state=42)
    train_idx, val_idx = next(sss.split(indices, labels))

    return Subset(dataset, train_idx), Subset(dataset, val_idx)

def load_fold_data(fold_idx, transform):
    fold_dir = f"{local_data_dir}/split_dataset/fold_{fold_idx}"
    return CustomDataset(fold_dir, transform=transform)

def evaluate_model(model, dataloader, device, criterion):
    model.eval()
    losses, preds, targets = [], [], []
    with torch.no_grad():
        for images, labels in dataloader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            losses.append(loss.item() * images.size(0))
            preds.extend(outputs.argmax(1).cpu().numpy())
            targets.extend(labels.cpu().numpy())
    avg_loss = sum(losses) / len(dataloader.dataset)
    return avg_loss, accuracy_score(targets, preds), precision_score(targets, preds, average='weighted'), recall_score(targets, preds, average='weighted'), f1_score(targets, preds, average='weighted'), confusion_matrix(targets, preds)

def print_class_distribution(dataset, name):
    if isinstance(dataset, Subset):
        labels = [dataset.dataset[idx][1] for idx in dataset.indices]
    elif isinstance(dataset, Dataset):
        labels = [dataset[i][1] for i in range(len(dataset))]
    else:
        raise ValueError("Unsupported dataset type")

    label_names = {0: 'Bleeding', 1: 'Ischemia', 2: 'Healthy'}
    counts = Counter(labels)
    print(f"\n{name.upper()} SET CLASS DISTRIBUTION:")
    for label, count in sorted(counts.items()):
        print(f"  {label_names[label]}: {count}")

def compute_class_weights(dataset):
    label_counts = Counter()
    for d in dataset.datasets:
        label_counts.update(d.labels)
    total = sum(label_counts.values())
    weights = [total / label_counts[i] for i in range(3)]
    weights[0] *= 0.8  # reduce Bleeding
    norm_weights = [w / sum(weights) for w in weights]
    return torch.tensor(norm_weights).float()


def print_class_distribution(dataset, name):
    if isinstance(dataset, Subset):
        labels = [dataset.dataset[idx][1] for idx in dataset.indices]
    elif isinstance(dataset, Dataset):
        labels = [dataset[i][1] for i in range(len(dataset))]
    else:
        raise ValueError("Unsupported dataset type")

    label_names = {0: 'Bleeding', 1: 'Ischemia', 2: 'Healthy'}
    counts = Counter(labels)
    print(f"\n{name.upper()} SET CLASS DISTRIBUTION:")
    for label, count in sorted(counts.items()):
        print(f"  {label_names[label]}: {count}")

#Loss Curve and Confusion Matrix
def plot_confusion_matrix(cm, model_idx, set_name, base_path):
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['Bleeding', 'Ischemia', 'Healthy'],
                yticklabels=['Bleeding', 'Ischemia', 'Healthy'])
    plt.title(f'Case {model_idx} - {set_name.capitalize()} Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.tight_layout()
    cm_filename = f"model_{model_idx}_{set_name}_confusion_matrix.png"
    plt.savefig(os.path.join(base_path, cm_filename))
    plt.show()
    plt.close()


def save_metrics_to_csv(model, loaders, device, criterion, model_idx, base_path):
    all_metrics = []
    for name, loader in loaders.items():
        loss, acc, prec, rec, f1, cm = evaluate_model(model, loader, device, criterion)
        all_metrics.append({
            'Model': model_idx,
            'Set': name,
            'Accuracy': acc,
            'Precision': prec,
            'Recall': rec,
            'F1_Score': f1
        })

    metrics_path = os.path.join(base_path, "metrics_combined.csv")
    df_metrics = pd.DataFrame(all_metrics)
    if os.path.exists(metrics_path):
        df_metrics.to_csv(metrics_path, mode='a', index=False, header=False)
    else:
        df_metrics.to_csv(metrics_path, index=False)

    return all_metrics

class CustomCallback:
    def __init__(self, model, optimizer, patience, stop_patience, threshold, factor, epochs):
        self.model = model
        self.optimizer = optimizer
        self.patience = patience
        self.stop_patience = stop_patience
        self.threshold = threshold
        self.factor = factor
        self.epochs = epochs
        self.count = 0
        self.stop_count = 0
        self.best_epoch = 1
        self.initial_lr = optimizer.param_groups[0]['lr']
        self.highest_tracc = 0.0
        self.lowest_vloss = np.inf
        self.best_weights = None
        self.start_time = None

    def on_train_begin(self):
        print(f"\n{'Epoch':<6} {'Train Loss':<12} {'Train Acc':<11} {'Val Loss':<10} {'Val Acc':<9} {'LR':<10}")

    def on_epoch_end(self, epoch, train_loss, val_loss, val_acc):
        current_lr = self.optimizer.param_groups[0]['lr']

        min_epochs = 10
        train_acc = self.highest_tracc

        if val_loss < self.lowest_vloss:
            self.lowest_vloss = val_loss
            self.best_weights = self.model.state_dict()
            self.count = 0
            self.stop_count = 0
            self.best_epoch = epoch + 1
        else:
            if epoch >= min_epochs:
                if self.count >= self.patience - 1:
                    current_lr = current_lr * self.factor
                    self.optimizer.param_groups[0]['lr'] = current_lr
                    self.count = 0
                    self.stop_count += 1
                else:
                    self.count += 1
            if train_acc > self.highest_tracc:
                self.highest_tracc = train_acc

        print(f"{epoch+1:<6} {train_loss:<12.4f} {train_acc*100:<11.2f} {val_loss:<10.4f} {val_acc*100:<9.2f} {current_lr:<10.6f}")

        if epoch >= min_epochs and self.stop_count > self.stop_patience - 1:
            print(f'training has been halted at epoch {epoch + 1} after {self.stop_patience} adjustments of learning rate with no improvement')
            return True
        return False

"""# **ResNet50**"""

def resnet50_model(num_classes=3):
    model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)
    model.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
    in_features = model.fc.in_features
    model.fc = nn.Sequential(
        nn.BatchNorm1d(in_features),
        nn.Linear(in_features, 256),
        nn.ReLU(),
        nn.Dropout(0.4),
        nn.Linear(256, num_classes)
    )
    return model

train_transform = A.Compose([
    A.HorizontalFlip(p=0.5),
    A.RandomBrightnessContrast(p=0.3),
    A.Rotate(limit=15, p=0.5),
    A.GaussNoise(p=0.2),
    A.RandomScale(scale_limit=0.1, p=0.3),
    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=10, p=0.3),
    A.Resize(224, 224),
    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),
    ToTensorV2(),
])

val_transform = A.Compose([
    A.Resize(224, 224),
    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),
    ToTensorV2(),
])

class CustomDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.image_paths = []
        self.labels = []
        self.transform = transform
        self.label_map = {'Bleeding': 0, 'Ischemia': 1, 'Healthy': 2}
        for category in self.label_map:
            category_dir = os.path.join(root_dir, category)
            for fname in os.listdir(category_dir):
                if fname.lower().endswith(('.png', '.jpg', '.jpeg')):
                    self.image_paths.append(os.path.join(category_dir, fname))
                    self.labels.append(self.label_map[category])

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        image = Image.open(self.image_paths[idx]).convert('L')
        image = np.array(image)
        image = np.stack([image]*3, axis=-1)
        if self.transform:
            image = self.transform(image=image)['image']
        return image, self.labels[idx]


def train_model(model_idx, test_fold, train_folds, base_path, device, lr=16e-5):
    test_dataset = load_fold_data(test_fold, val_transform)
    train_val_datasets = [load_fold_data(i, train_transform) for i in train_folds]
    train_val_dataset = ConcatDataset(train_val_datasets)

    train_dataset, val_dataset = stratified_split(train_val_dataset, val_ratio=0.15)

    print_class_distribution(train_dataset, "Train")
    print_class_distribution(val_dataset, "Validation")

    batch_size = 8
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)

    model = resnet50_model(num_classes=3).to(device)
    class_weights = compute_class_weights(train_val_dataset).to(device)
    criterion = nn.CrossEntropyLoss(weight=class_weights)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    callback = CustomCallback(model, optimizer, patience=3, stop_patience=5, threshold=0.9, factor=0.5, epochs=40)


    callback.on_train_begin()
    for epoch in range(40):
        model.train()
        train_loss = 0
        train_preds, train_targets = [], []

        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * images.size(0)
            train_preds.extend(outputs.argmax(1).cpu().numpy())
            train_targets.extend(labels.cpu().numpy())

        train_loss /= len(train_loader.dataset)
        train_acc = accuracy_score(train_targets, train_preds)

        val_loss, val_acc, val_prec, val_rec, val_f1, _ = evaluate_model(model, val_loader, device, criterion)

        callback.highest_tracc = train_acc
        stop = callback.on_epoch_end(epoch, train_loss, val_loss, val_acc)
        if stop:
            break

    if callback.best_weights is not None:
        model.load_state_dict(callback.best_weights)

    model_name = f"resnet50_model_{model_idx}_final.pth"
    model_path = os.path.join(base_path, model_name)
    torch.save(model.state_dict(), model_path)
    print(f"Saved final model to {model_path}")

    loaders = {'train': train_loader, 'val': val_loader, 'test': test_loader}
    save_metrics_to_csv(model, loaders, device, criterion, model_idx, base_path)

def train_all_models():
    torch.manual_seed(42)
    np.random.seed(42)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    base_path = "/content/drive/My Drive/Neural Networks/ResNet50"
    os.makedirs(base_path, exist_ok=True)
    for model_idx in range(5):
        print(f"\n[Model {model_idx}] Training with Fold {model_idx} as test")
        test_fold = model_idx
        train_folds = [i for i in range(5) if i != model_idx]
        train_model(model_idx, test_fold, train_folds, base_path, device)

if __name__ == "__main__":
    train_all_models()

"""#**EfficientNet-B0**"""

train_transform = A.Compose([
    A.HorizontalFlip(p=0.5),
    A.RandomBrightnessContrast(p=0.2),
    A.Rotate(limit=15, p=0.3),
    A.GaussNoise(p=0.2),
    A.RandomScale(scale_limit=0.1, p=0.2),
    A.Resize(224, 224),
    A.Normalize(mean=(0.5,), std=(0.5,)),
    ToTensorV2(),
])

val_transform = A.Compose([
    A.Resize(224, 224),
    A.Normalize(mean=(0.5,), std=(0.5,)),
    ToTensorV2(),
])

class CustomDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.image_paths = []
        self.labels = []
        self.transform = transform
        self.label_map = {'Bleeding': 0, 'Ischemia': 1, 'Healthy': 2}
        for category in self.label_map:
            category_dir = os.path.join(root_dir, category)
            for fname in os.listdir(category_dir):
                if fname.lower().endswith(('.png', '.jpg', '.jpeg')):
                    self.image_paths.append(os.path.join(category_dir, fname))
                    self.labels.append(self.label_map[category])

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        image = Image.open(self.image_paths[idx]).convert('L')
        image = np.array(image)
        if self.transform:
            image = self.transform(image=image)['image']
        return image, self.labels[idx]


def efficientnet_b0_model(num_classes=3):
    model = models.efficientnet_b0(weights="IMAGENET1K_V1")
    model.features[0][0] = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False)
    model.classifier = nn.Sequential(
        nn.BatchNorm1d(model.classifier[1].in_features, momentum=0.99, eps=0.001),
        nn.Linear(model.classifier[1].in_features, 256),
        nn.ReLU(),
        nn.Dropout(0.5),
        nn.Linear(256, num_classes)
    )
    return model

def train_model(model_idx, test_fold, train_folds, base_path, device, lr=0.0002):
    test_dataset = load_fold_data(test_fold, val_transform)
    train_val_datasets = [load_fold_data(i, train_transform) for i in train_folds]
    train_val_dataset = ConcatDataset(train_val_datasets)

    train_dataset, val_dataset = stratified_split(train_val_dataset, val_ratio=0.15)

    print_class_distribution(train_dataset, "Train")
    print_class_distribution(val_dataset, "Validation")

    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)

    model = efficientnet_b0_model(num_classes=3)
    for param in model.parameters():
        param.requires_grad = True

    model = model.to(device)
    criterion = nn.CrossEntropyLoss(weight=compute_class_weights(train_val_dataset).to(device))
    optimizer = optim.Adam(model.parameters(), lr=lr)
    callback = CustomCallback(model, optimizer, patience=3, stop_patience=5, threshold=0.9, factor=0.5, epochs=40)
    train_losses = []
    val_losses = []
    metrics = []

    callback.on_train_begin()
    for epoch in range(40):
        model.train()
        train_loss_sum = 0
        train_preds, train_targets = [], []
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            l2_reg = sum(torch.norm(param, 2) for param in model.classifier.parameters()) * 0.001
            l1_reg = sum(torch.norm(param, 1) for param in model.classifier.parameters()) * 0.001
            total_loss = loss + l2_reg + l1_reg
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            train_loss_sum += loss.item() * images.size(0)
            train_preds.extend(outputs.argmax(1).cpu().numpy())
            train_targets.extend(labels.cpu().numpy())

        train_loss = train_loss_sum / len(train_loader.dataset)
        train_acc = accuracy_score(train_targets, train_preds)
        train_losses.append(train_loss)

        val_loss, val_acc, val_prec, val_rec, val_f1, cm = evaluate_model(model, val_loader, device, criterion)
        val_losses.append(val_loss)

        callback.highest_tracc = train_acc
        stop = callback.on_epoch_end(epoch, train_loss, val_loss, val_acc)
        metrics.append({
            'Epoch': epoch + 1,
            'Train_Loss': train_loss,
            'Val_Loss': val_loss,
            'Val_Accuracy': val_acc,
            'Val_Precision': val_prec,
            'Val_Recall': val_rec,
            'Val_F1_Score': val_f1,
        })
        if stop:
            break

    final_model_path = os.path.join(base_path, f"efficientnet_b0_model_{model_idx}_final.pth")
    torch.save(model.state_dict(), final_model_path)
    print(f"Saved final model to {final_model_path}")

    metrics_df = pd.DataFrame(metrics).assign(Model=model_idx)
    metrics_csv = os.path.join(base_path, f"loss_model_{model_idx}.csv")
    metrics_df.to_csv(metrics_csv, index=False)
    print(f"Saved metrics to {metrics_csv}")

    model.load_state_dict(callback.best_weights if callback.best_weights else model.state_dict())

    loaders = {'train': train_loader, 'val': val_loader, 'test': test_loader}
    save_metrics_to_csv(model, loaders, device, criterion, model_idx, base_path)


def train_all_models():
    torch.manual_seed(42)
    np.random.seed(42)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    base_path = "/content/drive/My Drive/Neural Networks/EfficientNetB0"
    os.makedirs(base_path, exist_ok=True)
    for model_idx in range(5):
        print(f"\n[Model {model_idx}] Training with Fold {model_idx} as test")
        test_fold = model_idx
        train_folds = [i for i in range(5) if i != model_idx]
        train_model(model_idx, test_fold, train_folds, base_path, device)

if __name__ == "__main__":
    train_all_models()

"""#**DensNet121***"""

# Define transforms for grayscale
train_transform = A.Compose([
    A.HorizontalFlip(p=0.5),
    A.RandomBrightnessContrast(p=0.3),
    A.Rotate(limit=15, p=0.5),
    A.GaussNoise(p=0.2),
    A.RandomScale(scale_limit=0.1, p=0.3),
    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=10, p=0.3),
    A.Resize(224, 224),
    A.Normalize(mean=(0.5,), std=(0.5,)),
    ToTensorV2(),
])

val_transform = A.Compose([
    A.Resize(224, 224),
    A.Normalize(mean=(0.5,), std=(0.5,)),
    ToTensorV2(),
])

# Custom Dataset for grayscale
class CustomDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.image_paths = []
        self.labels = []
        self.transform = transform
        self.label_map = {'Bleeding': 0, 'Ischemia': 1, 'Healthy': 2}
        for category in self.label_map:
            category_dir = os.path.join(root_dir, category)
            for fname in os.listdir(category_dir):
                if fname.lower().endswith(('.png', '.jpg', '.jpeg')):
                    self.image_paths.append(os.path.join(category_dir, fname))
                    self.labels.append(self.label_map[category])

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        image = Image.open(self.image_paths[idx]).convert('L')
        image = np.array(image)[:, :, np.newaxis]  # Shape: (H, W, 1)
        if self.transform:
            image = self.transform(image=image)['image']
        return image, self.labels[idx]

# DenseNet-121 model for grayscale
def densenet121_model(num_classes=3):
    model = models.densenet121(weights='IMAGENET1K_V1')
    # Modify first conv layer for 1-channel input
    original_conv0_weight = model.features.conv0.weight  # Shape: (64, 3, 7, 7)
    new_conv0 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)
    with torch.no_grad():
        new_conv0.weight = nn.Parameter(original_conv0_weight[:, 0:1, :, :])  # Shape: (64, 1, 7, 7)
    model.features.conv0 = new_conv0
    # Modify classifier
    in_features = model.classifier.in_features
    model.classifier = nn.Sequential(
        nn.BatchNorm1d(in_features),
        nn.Linear(in_features, 256),
        nn.ReLU(),
        nn.Dropout(0.4),
        nn.Linear(256, num_classes)
    )
    return model

# Train model
def train_model(model_idx, test_fold, train_folds, base_path, device, lr=0.00008):
    test_dataset = load_fold_data(test_fold, val_transform)
    train_val_datasets = [load_fold_data(i, train_transform) for i in train_folds]
    train_val_dataset = ConcatDataset(train_val_datasets)

    train_dataset, val_dataset = stratified_split(train_val_dataset, val_ratio=0.15)

    print_class_distribution(train_dataset, "Train")
    print_class_distribution(val_dataset, "Validation")

    batch_size = 16
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)

    model = densenet121_model(num_classes=3).to(device)
    class_weights = compute_class_weights(train_val_dataset).to(device)
    criterion = nn.CrossEntropyLoss(weight=class_weights)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    callback = CustomCallback(model, optimizer, patience=3, stop_patience=5, threshold=0.9, factor=0.5, epochs=40)

    train_losses, val_losses = [], []

    callback.on_train_begin()
    for epoch in range(40):
        model.train()
        train_loss = 0
        train_preds, train_targets = [], []

        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)  # Fixed bug
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * images.size(0)
            train_preds.extend(outputs.argmax(1).cpu().numpy())
            train_targets.extend(labels.cpu().numpy())

        train_loss /= len(train_loader.dataset)
        train_acc = accuracy_score(train_targets, train_preds)
        train_losses.append(train_loss)

        val_loss, val_acc, val_prec, val_rec, val_f1, _ = evaluate_model(model, val_loader, device, criterion)
        val_losses.append(val_loss)

        callback.highest_tracc = train_acc
        stop = callback.on_epoch_end(epoch, train_loss, val_loss, val_acc)
        if stop:
            break

    if callback.best_weights is not None:
        model.load_state_dict(callback.best_weights)

    model_name = f"densenet121_model_{model_idx}_final.pth"
    model_path = os.path.join(base_path, model_name)
    torch.save(model.state_dict(), model_path)

    loaders = {'train': train_loader, 'val': val_loader, 'test': test_loader}
    save_metrics_to_csv(model, loaders, device, criterion, model_idx, base_path)

# Train all models
def train_all_models():
    torch.manual_seed(42)
    np.random.seed(42)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    base_path = "/content/drive/My Drive/Neural Networks/DensNet121"
    os.makedirs(base_path, exist_ok=True)
    for model_idx in range(5):
        print(f"\n[Model {model_idx}] Training with Fold {model_idx} as test")
        test_fold = model_idx
        train_folds = [i for i in range(5) if i != model_idx]
        train_model(model_idx, test_fold, train_folds, base_path, device)

if __name__ == "__main__":
    train_all_models()

"""#**Confusion Matrices**"""

def print_confusion_matrixes(arch_name, model_dir, model_fn, val_transform, load_fold_data, evaluate_model, save_metrics_to_csv):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    criterion = nn.CrossEntropyLoss()

    print(f"ğŸ§  Î‘ÏÏ‡Î¹Ï„ÎµÎºÏ„Î¿Î½Î¹ÎºÎ®: {arch_name.upper()}")

    for i in range(5):
        print(f"\nğŸ” Î‘Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ· ÎœÎ¿Î½Ï„Î­Î»Î¿Ï… {i} (Fold {i})")

        model = model_fn(num_classes=3)
        model_path = os.path.join(model_dir, f"model_{i}_final.pth")
        model.load_state_dict(torch.load(model_path, map_location=device))
        model.to(device)

        test_dataset = load_fold_data(i, val_transform)
        test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)

        loss, acc, prec, rec, f1, cm = evaluate_model(model, test_loader, device, criterion)

        print(f"Accuracy: {acc:.6f} | Precision: {prec:.6f} | Recall: {rec:.6f} | F1: {f1:.6f}")

        # Plot confusion matrix inside this function:
        plt.figure(figsize=(6, 5))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=['Bleeding', 'Ischemia', 'Healthy'],
                    yticklabels=['Bleeding', 'Ischemia', 'Healthy'])
        plt.title(f'Case {i} - Test Confusion Matrix')
        plt.xlabel('Predicted')
        plt.ylabel('True')
        plt.tight_layout()
        cm_filename = f"model_{i}_test_confusion_matrix.png"
        plt.savefig(os.path.join(model_dir, cm_filename))
        plt.show()
        plt.close()

        save_metrics_to_csv(model, {'test': test_loader}, device, criterion, model_idx=i, base_path=model_dir)

"""Confusion matrix for each ResNet50 model"""

print_confusion_matrixes(
    arch_name="resnet50",
    model_dir="/content/drive/My Drive/Neural Networks/ResNet50",
    model_fn=resnet50_model,
    val_transform=val_transform,
    load_fold_data=load_fold_data,
    evaluate_model=evaluate_model,
    save_metrics_to_csv=save_metrics_to_csv
)

"""Confucion Matrix for each EfficientNet-B0 model"""

print_confusion_matrixes(
    arch_name="efficientnet_b0",
    model_dir="/content/drive/My Drive/Neural Networks/EfficientNetB0",
    model_fn=efficientnet_b0_model,
    val_transform=val_transform,
    load_fold_data=load_fold_data,
    evaluate_model=evaluate_model,
    save_metrics_to_csv=save_metrics_to_csv
)

"""Confucion Matrix for each DensNet121 model"""

print_confusion_matrixes(
    arch_name="densnet121",
    model_dir="/content/drive/My Drive/Neural Networks/DensNet121",
    model_fn=densenet121_model,
    val_transform=val_transform,
    load_fold_data=load_fold_data,
    evaluate_model=evaluate_model,
    save_metrics_to_csv=save_metrics_to_csv
)

"""#**Accuracy and Loss Curves**"""

# Load metrics from CSV
def load_metrics_from_csv(csv_path, model_idx):
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"CSV file not found at {csv_path}")
    df = pd.read_csv(csv_path)
    model_df = df[df['model'] == model_idx]
    if model_df.empty:
        raise ValueError(f"No data found for Model {model_idx} in {csv_path}")
    return {
        'train_losses': model_df['loss'].tolist(),
        'val_losses': model_df['val_loss'].tolist(),
        'train_accuracies': model_df['accuracy'].tolist(),
        'val_accuracies': model_df['val_accuracy'].tolist(),
        'epochs': model_df['epoch'].tolist()
    }

def plot_combined_curve(train_losses, val_losses, train_accuracies, val_accuracies, epochs, model_idx, base_path):
    plt.figure(figsize=(16, 5))  # Double width for side-by-side plots

    # Loss subplot (left)
    plt.subplot(1, 2, 1)
    plt.plot(epochs, train_losses, label='Train Loss')
    plt.plot(epochs, val_losses, label='Val Loss')
    plt.title(f'Case {model_idx} - Loss Curve')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)

    # Accuracy subplot (right)
    plt.subplot(1, 2, 2)
    plt.plot(epochs, train_accuracies, label='Train Accuracy')
    plt.plot(epochs, val_accuracies, label='Val Accuracy')
    plt.title(f'Case {model_idx} - Accuracy Curve')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    os.makedirs(base_path, exist_ok=True)
    plt.savefig(os.path.join(base_path, f"model_{model_idx}_combined_curve.png"))
    plt.show()
    plt.close()

# Generate plots for all models
def generate_plots_for_all_models(csv_path, plots_path):
    for model_idx in range(5):  # Models 0-4
        try:
            metrics = load_metrics_from_csv(csv_path, model_idx)
            plot_combined_curve(
                metrics['train_losses'], metrics['val_losses'],
                metrics['train_accuracies'], metrics['val_accuracies'],
                metrics['epochs'], model_idx, plots_path
            )
            print(f"Generated combined plot for Model {model_idx}")
        except (FileNotFoundError, ValueError) as e:
            print(f"Error for Model {model_idx}: {e}")

"""Accuracy and Loss Curves - ResNet50"""

base_path = "/content/drive/My Drive/Neural Networks/ResNet50"
csv_filename = "resnet_train_data.csv"
csv_path = os.path.join(base_path, csv_filename)
plots_path = os.path.join(base_path, "plots")

generate_plots_for_all_models(csv_path, plots_path)

"""Accuracy and Loss Curves - EfficientNet-B0"""

base_path = "/content/drive/My Drive/Neural Networks/EfficientNetB0"
csv_filename = "efficientnet_train_data.csv"
csv_path = os.path.join(base_path, csv_filename)
plots_path = os.path.join(base_path, "plots")

generate_plots_for_all_models(csv_path, plots_path)

"""Accuracy and Loss Curves - DensNet121"""

base_path = "/content/drive/My Drive/Neural Networks/DensNet121"
csv_filename = "densnet_train_data.csv"
csv_path = os.path.join(base_path, csv_filename)
plots_path = os.path.join(base_path, "plots")

generate_plots_for_all_models(csv_path, plots_path)